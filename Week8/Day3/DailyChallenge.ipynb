{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10d7937d",
      "metadata": {
        "id": "10d7937d"
      },
      "source": [
        "# Daily Challenge: Creating A Text Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4905c83",
      "metadata": {
        "id": "b4905c83"
      },
      "source": [
        "## Preprocess Text Data:\n",
        "1. Access the text\n",
        "2. Create a preprocess function to clean non-words not relevant to the context using regular expressions\n",
        "3. Split the text: how you should split the text considering the final objective of this project is to create a sentence?\n",
        "- return the splited text as the corpus\n",
        "\n",
        "4. print the first 200 characteres of the corpus.\n",
        "Are there parts of the text that are not relevant to the analysis? If so, the function should remove them as well.\n",
        "\n",
        "hint: you can use slicing to start and stop the text where you need (ignoring autoral credits in the begining and end) looking for the following phrases:\n",
        "\n",
        "*** START\n",
        "*** END\n",
        "\n",
        "5. Using Tokenizer(), create the vocabulary and a variable called total_words which will be the lenght of tokenizer.word_index + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df09ffa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4df09ffa",
        "outputId": "6f15021b-7bde-4857-838f-c887be1c9e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb9P6xaQwT-z",
        "outputId": "915e8514-335b-42ef-f251-434e01334d40"
      },
      "id": "wb9P6xaQwT-z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "AN6WFj2fmq_9"
      },
      "id": "AN6WFj2fmq_9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "-4Q3xh6DkPCo"
      },
      "id": "-4Q3xh6DkPCo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2755c92c",
      "metadata": {
        "id": "2755c92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffbab52-b1af-4257-cff0-470f7169a81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tensorflow import keras\n",
        "#nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd9fd57",
      "metadata": {
        "id": "bcd9fd57"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
        "response = requests.get(url)\n",
        "text1 = response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove non-alphabetic characters and extra whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Split the text into sentences\n",
        "def split_text_into_sentences(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Preprocess the raw text\n",
        "cleaned_text = preprocess_text(text1)\n",
        "sentences = split_text_into_sentences(cleaned_text)\n",
        "\n",
        "# Print the first 200 characters of the corpus\n",
        "corpus = \" \".join(sentences)\n",
        "print(\"First 200 characters of the corpus:\")\n",
        "print(corpus[:200])\n",
        "\n",
        "# Create vocabulary and calculate total_words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"\\nTotal number of words in the vocabulary:\", total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmqk2qzEwQ71",
        "outputId": "61fd15ca-8fd2-42de-cec0-b4c8b60d7856"
      },
      "id": "Rmqk2qzEwQ71",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 200 characters of the corpus:\n",
            "The Project Gutenberg eBook of Alices Adventures in Wonderland This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restric\n",
            "\n",
            "Total number of words in the vocabulary: 3193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0392c02f",
      "metadata": {
        "id": "0392c02f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Neural Network Model:\n",
        "Define a Sequential model in Keras.\n",
        "Add an Embedding layer for text representation.\n",
        "Include appropriate RNN layers for processing the sequences.\n",
        "Add a Dense layer for output prediction.\n",
        "Utilize Dropout for regularization."
      ],
      "metadata": {
        "id": "LvjbFiVKVH4c"
      },
      "id": "LvjbFiVKVH4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9956346f",
      "metadata": {
        "id": "9956346f"
      },
      "outputs": [],
      "source": [
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Create n-gram sequences for input data\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Separate into input and output\n",
        "X, y = padded_input_sequences[:, :-1], padded_input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile And Train The Model:\n",
        "Compile the model with an appropriate optimizer and loss function.\n",
        "Train the model on the prepared data.\n",
        "Implement EarlyStopping to prevent overfitting."
      ],
      "metadata": {
        "id": "l1MR7YvWV-Yt"
      },
      "id": "l1MR7YvWV-Yt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8872fee0",
      "metadata": {
        "id": "8872fee0"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model if needed\n",
        "# evaluation = model.evaluate(X_test, y_test)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e58dad0",
      "metadata": {
        "id": "0e58dad0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate The Model’s Performance On Test Data:\n",
        "Create generate_text() function with the appropriate arguments. The function should to preprocess the seed_text, predict the next_words and add then after the seed text in an output string.\n",
        "\n",
        "Experiment with different model architectures, hyperparameters, and preprocessing techniques to improve performance. ​(Try using LSTM and then try GRU)"
      ],
      "metadata": {
        "id": "hG-kLV2Wj4Ay"
      },
      "id": "hG-kLV2Wj4Ay"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40aebba3",
      "metadata": {
        "id": "40aebba3"
      },
      "outputs": [],
      "source": [
        "def generate_text_lstm(model, tokenizer, seed_text, next_words, max_sequence_length):\n",
        "    output_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # Pad the tokenized seed text\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predicted_index = model.predict_classes(token_list, verbose=0)\n",
        "        # Convert index to word\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index[0], '')\n",
        "        # Update the seed text for the next iteration\n",
        "        seed_text += \" \" + predicted_word\n",
        "        # Add the predicted word to the output text\n",
        "        output_text += \" \" + predicted_word\n",
        "    return output_text\n",
        "\n",
        "# Example usage for LSTM\n",
        "seed_text_example = \"Alice\"\n",
        "generated_text_lstm = generate_text_lstm(model, tokenizer, seed_text_example, next_words=20, max_sequence_length=max_sequence_length)\n",
        "print(\"Generated Text (LSTM):\", generated_text_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e23c74",
      "metadata": {
        "id": "38e23c74"
      },
      "outputs": [],
      "source": [
        "def generate_text_gru(model, tokenizer, seed_text, next_words, max_sequence_length):\n",
        "    output_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # Pad the tokenized seed text\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predicted_index = model.predict_classes(token_list, verbose=0)\n",
        "        # Convert index to word\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index[0], '')\n",
        "        # Update the seed text for the next iteration\n",
        "        seed_text += \" \" + predicted_word\n",
        "        # Add the predicted word to the output text\n",
        "        output_text += \" \" + predicted_word\n",
        "    return output_text\n",
        "\n",
        "# Example usage for GRU\n",
        "generated_text_gru = generate_text_gru(model, tokenizer, seed_text_example, next_words=20, max_sequence_length=max_sequence_length)\n",
        "print(\"Generated Text (GRU):\", generated_text_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87b4e6b",
      "metadata": {
        "id": "a87b4e6b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}